<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Publications | IMM Laboratory</title>

<link rel="icon" href="/preview/pr-2/images/icon.png">

<meta name="title" content="Publications">
<meta name="description" content="">

<meta property="og:title" content="Publications">
<meta property="og:site_title" content="IMM Laboratory">
<meta property="og:description" content="">
<meta property="og:url" content="">
<meta property="og:image" content="/preview/pr-2/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Publications">
<meta property="twitter:description" content="">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/preview/pr-2/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Publications",
    "description": "",
    "headline": "Publications",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/preview/pr-2/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/preview/pr-2/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/preview/pr-2/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/all.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/background.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/body.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/button.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/card.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/code.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/float.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/font.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/form.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/header.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/image.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/link.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/list.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/main.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/section.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/table.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/preview/pr-2/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/preview/pr-2/_scripts/anchors.js"></script>

  <script src="/preview/pr-2/_scripts/dark-mode.js"></script>

  <script src="/preview/pr-2/_scripts/fetch-tags.js"></script>

  <script src="/preview/pr-2/_scripts/search.js"></script>

  <script src="/preview/pr-2/_scripts/site-search.js"></script>

  <script src="/preview/pr-2/_scripts/tooltip.js"></script>


</head>
  <body>
    





<header class="background" style="--image: url('/preview/pr-2/images/background.jpg')">
  <a href="/preview/pr-2/" class="home">
    
      <span class="logo">
        
          <img src="/preview/pr-2/images/logo.png" alt="logo">
        
      </span>
    
    
      <span class="title" data-tooltip="Home">
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/preview/pr-2/Research/" data-tooltip="Research aims and studies">
          Research
        </a>
      
    
      
        <a href="/preview/pr-2/Publications/" data-tooltip="Research papers and PDFs">
          Publications
        </a>
      
    
      
        <a href="/preview/pr-2/People/" data-tooltip="About our team">
          People
        </a>
      
    
      
        <a href="/preview/pr-2/Opportunities/" data-tooltip="Research participants and open positions">
          Opportunities
        </a>
      
    
      
        <a href="/preview/pr-2/Gallery/" data-tooltip="Lab photos and events">
          Gallery
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 id="publications">
<i class="icon fa-solid fa-feather-pointed"></i>Publications</h1>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<div class="tags" data-link="/preview/pr-2/Publications/">
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20perceptual%20grouping%22" class="tag" data-tooltip='Show items with the tag "perceptual grouping"'>
        perceptual grouping
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20visuomotor%20learning%22" class="tag" data-tooltip='Show items with the tag "visuomotor learning"'>
        visuomotor learning
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20hand%20movement%20tracking%22" class="tag" data-tooltip='Show items with the tag "hand movement tracking"'>
        hand movement tracking
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20MEG%22" class="tag" data-tooltip='Show items with the tag "MEG"'>
        MEG
      </a>
    
      <a href="/preview/pr-2/Publications/?search=%22tag:%20eye%20movement%20tracking%22" class="tag" data-tooltip='Show items with the tag "eye movement tracking"'>
        eye movement tracking
      </a>
    
  </div>

<div class="search-info"></div>

<h3 id="2023">2023</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2023/08/31/publication-20.html">The effect of masks on the emotion perception of a facial crowd</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Cho, J., Im, H.Y., Yoon, Y.J., Joo, S.J., and Chong, S.C.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>August 31, 2023</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2023</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractThe present study investigated the effect of facial masks on people s ability to perceive emotions in crowds  We presented faces with the bottom halves occluded by masks or full faces without occlusion  In two sequentially presented crowds  we varied the number of faces  emotional valence  and intensity of facial expressions  examining the impact of masks on the perception of crowd emotion  Participants reported which of the two crowds they would avoid based on the crowds  average emotions  The participants  ability to judge the average emotion of a crowd  especially a crowd expressing happiness  was impaired when the crowd wore masks  For faces covered by masks  crowd emotion judgments were more negatively biased than those without masks  However  participants could still distinguish the emotional intensities of a crowd wearing masks above chance  Additionally  participants responded more quickly to a crowd with more people without compromising accuracy  despite the perceptual challenges imposed by facial masks  Our results suggest that under ambiguous social situations in which individuals  emotions are partially hidden by masks  a large group may provide stronger social cues than a small group  thereby promoting communication and regulating social behaviors Cho  J   Im  H Y   Yoon  Y J  et al  The effect of masks on the emotion perception of a facial crowd  Sci Rep 13  14274  2023   https   doi org 10 1038 s41598 023 41366 0    PDF ">
    
  </p>
</div>

<h3 id="2022">2022</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2022/09/01/publication-19.html">Inconsistent attentional contexts impair relearning following gradual visuomotor adaptation</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H. Y., Liddy, J. J., &amp; Song, J.-H.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>September 01, 2022</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2022</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20visuomotor%20learning%22" class="tag" data-tooltip='Show items with the tag "visuomotor learning"'>
        visuomotor learning
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20hand%20movement%20tracking%22" class="tag" data-tooltip='Show items with the tag "hand movement tracking"'>
        hand movement tracking
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractOne of the brain s primary functions is to promote actions in dynamic  distracting environments  Because distractions divert attention from our primary goals  we must learn to maintain accurate actions under sensory and cognitive distractions  Visuomotor adaptation is a learning process that restores performance when sensorimotor capacities or environmental conditions are abruptly or gradually altered  Prior work showed that learning to counteract an abrupt perturbation under a particular single  or dual task setting  i e   attentional context  was associated with better recall under the same conditions  This suggested that the attentional context was encoded during adaptation and used as a recall cue  The current study investigated whether the attentional context  i e   single vs  dual task  also affected adaptation and recall to a gradual perturbation  which limited awareness of movement errors  During adaptation  participants moved a cursor to a target while learning to counteract a visuomotor rotation that increased from 0 to 45 by 0 3 each trial  with or without performing a secondary task  Relearning was impaired when the attentional context was different between adaptation and recall  experiment 1   even when the exposure to the attentional context was limited to the early or late half of adaptation  experiment 2   Changing the secondary task did not affect relearning  indicating that the attentional context  rather than specific stimuli or tasks  was associated with better recall performance  experiment 3   These findings highlight the importance of cognitive factors  such as attention  in visuomotor adaptation and have implications for learning and rehabilitation paradigms NEW  amp  NOTEWORTHY Adaptation acquired under single  or dual task setting  which created an undivided or divided attentionalcontext  respectively  was impaired when relearning occurred under different conditions  i e   shifting from a dual to single task  Changes to the attentional context impaired relearning when the initial adaptation was to a gradual perturbation  Explicit awareness of the perturbation was not necessary for this effect to be robust  nor was the effect attributable to changes in the secondary task requirements  Im  H  Y   Liddy  J  J    amp  Song  J  H   2022   Inconsistent attentional contexts impair relearning following gradual visuomotor adaptation  Journal of Neurophysiology  128 3   527 542  https   doi org 10 1152 jn 00463 2021   PDF ">
    
  </p>
</div>

<h3 id="2021">2021</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2021/02/03/publication-18.html">Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H. Y., Cushing, C., Ward, N., &amp; Kveraga, K</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>February 03, 2021</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2021</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20MEG%22" class="tag" data-tooltip='Show items with the tag "MEG"'>
        MEG
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractReading the prevailing emotion of groups of people   crowd emotion   is critical to understanding their overall intention and disposition  It alerts us to potential dangers  such as angry mobs or panicked crowds  giving us time to escape  A critical aspect of processing crowd emotion is that it must occur rapidly  because delays often are costly  Although knowing the timing of neural events is crucial for understanding how the brain guides behaviors using coherent signals from a glimpse of multiple faces  this information is currently lacking in the literature on face ensemble coding  Therefore  we used magnetoencephalography to examine the neurodynamics in the dorsal and ventral visual streams and the periamygdaloid cortex to compare perception of groups of faces versus individual faces  Forty six participants compared two groups of four faces or two individual faces with varying emotional expressions and chose which group or individual they would avoid  We found that the dorsal stream was activated as early as 68 msec after the onset of stimuli containing groups of faces  In contrast  the ventral stream was activated later and predominantly for individual face stimuli  The latencies of the dorsal stream activation peaks correlated with participants  response times for facial crowds  We also found enhanced connectivity earlier between the periamygdaloid cortex and the dorsal stream regions for crowd emotion perception  Our findings suggest that ensemble coding of facial crowds proceeds rapidly and in parallel by engaging the dorsal stream to mediate adaptive social behaviors  via a distinct route from single face perception Im  H Y   Cushing  C   Ward  N    amp  Kveraga  K   2021   Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds and individuals  a MEG study  Cognitive  Affective  and Behavioral Neuroscience   PDF ">
    
  </p>
</div>

<h3 id="2020">2020</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2020/02/25/publication-16.html">Fast saccadic and manual responses to faces presented to the koniocellular visual pathway</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Kveraga, K., Im, H. Y., Ward, N., &amp; Adams, R. B. Jr.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>February 25, 2020</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2020</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20eye%20movement%20tracking%22" class="tag" data-tooltip='Show items with the tag "eye movement tracking"'>
        eye movement tracking
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractThe parallel pathways of the human visual system differ in their tuning to luminance  color  and spatial frequency  These attunements recently have been shown to propagate to differential processing of higher order stimuli  facial threat cues  in the magnocellular  M  and parvocellular  P  pathways  with greater sensitivity to clear and ambiguous threat  respectively  The role of the third  koniocellular  K  pathway in facial threat processing  however  remains unknown  To address this gap in knowledge  we briefly presented peripheral face stimuli psychophysically biased towards M  P  or K pathways  Observers were instructed to report via a key press whether the face was angry or neutral while their eye movements and manual responses were recorded  We found that short latency saccades were made more frequently to faces presented in the K channel than to P or M channels  Saccade latencies were not significantly modulated by expressive and identity cues  In contrast  manual response latencies and accuracy were modulated by both pathway biasing and by interactions of facial expression with facial masculinity  such that angry male faces elicited the fastest  and angry female faces  the least accurate  responses  We conclude that face stimuli can evoke fast saccadic and manual responses when projected to the K pathway Kveraga  K   Im  H Y   Ward  N    amp  Adams  R B Jr   2020   Fast saccadic and manual responses to faces presented to the koniocellular visual pathway  Journal of Vision  20 2  9  doi  10 1167 jov 20 2 9   PDF ">
    
  </p>
</div>

<h3 id="2019">2019</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2019/04/23/publication-15.html">Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Adams, R. B. Jr., Im, H. Y., Cushing, C., Boshyan, J., Ward, N., Albohn, D. N., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>April 23, 2019</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2019</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractRecently  speed of presentation of facially expressive stimuli was found to influence the processing of compound threat cues  e g   anger fear gaze   For instance  greater amygdala responses were found to clear  e g   direct gaze anger averted gaze fear  versus ambiguous  averted gaze anger direct gaze fear  combinations of threat cues when rapidly presented  33 and 300ms   but greater to ambiguous versus clear threat cues when presented for more sustained durations  1  1 5  and 2 s   A working hypothesis was put forth  Adams et al   2012  that these effects were due to differential magnocellular versus parvocellular pathways contributions to the rapid versus sustained processing of threat  respectively  To test this possibility directly here  we restricted visual stream processing in the fMRI environment using facially expressive stimuli specifically designed to bias visual input exclusively to the magnocellular versus parvocellular pathways  We found that for magnocellular biased stimuli  activations were predominantly greater to clear versus ambiguous threat gaze pairs  on par with that previously found for rapid presentations of threat cues   whereas activations to ambiguous versus clear threat gaze pairs were greater for parvocellular biased stimuli  on par with that previously found for sustained presentations   We couch these findings in an adaptive dual process account of threat perception and highlight implications for other dual process models within psychology Adams  R B Jr   Im  H Y   Cushing  C   Boshyan  J   Ward  N   Albohn  D N    amp  Kveraga  K   2019   Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat  Progress in Brain Research  247  71 87   PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2019/02/13/publication-14.html">Magnocellular and parvocellular pathway contributions to facial threat cue processing</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Cushing, C., Im, H. Y., Adams, R. B. Jr., Ward, N., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>February 13, 2019</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2019</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20MEG%22" class="tag" data-tooltip='Show items with the tag "MEG"'>
        MEG
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractHuman faces evolved to signal emotions  with their meaning contextualized by eye gaze  For instance  a fearful expression paired with averted gaze clearly signals both presence of threat and its probable location  Conversely  direct gaze paired with facial fear leaves the source of the fear evoking threat ambiguous  Given that visual perception occurs in parallel streams with different processing emphases  our goal was to test a recently developed hypothesis that clear and ambiguous threat cues would differentially engage the magnocellular  M  and parvocellular  P  pathways  respectively  We employed two tone face images to characterize the neurodynamics evoked by stimuli that were biased toward M or P pathways  Human observers  N   57  had to identify the expression of fearful or neutral faces with direct or averted gaze while their magnetoencephalogram was recorded  Phase locking between the amygdaloid complex  orbitofrontal cortex  OFC  and fusiform gyrus increased early  0 300 ms  for M biased clear threat cues  averted gaze fear  in the β band  13 30 Hz  while P biased ambiguous threat cues  direct gaze fear  evoked increased θ  4 8 Hz  phase locking in connections with OFC of the right hemisphere  We show that M and P pathways are relatively more sensitive toward clear and ambiguous threat processing  respectively  and characterize the neurodynamics underlying emotional face processing in the M and P pathways Cushing  C   Im  H Y   Adams  R B Jr   Ward  N    amp  Kveraga  K   2019   Magnocellular and parvocellular pathway contributions to facial threat cue processing  Social Cognitive and Affective Neuroscience  14  151 162    PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2019/01/25/publication-13.html">Spatial and feature-based attention to expressive faces</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Kveraga, K., De Vito, D., Cushing, C., Im, H. Y., Albohn, D. N., &amp; Adams, R. B. Jr.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>January 25, 2019</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2019</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractFacial emotion is an important cue for deciding whether an individual is potentially helpful or harmful  However  facial expressions are inherently ambiguous and observers typically employ other cues to categorize emotion expressed on the face  such as race  sex  and context  Here  we explored the effect of increasing or reducing different types of uncertainty associated with a facial expression that is to be categorized  On each trial  observers responded according to the emotion and location of a peripherally presented face stimulus and were provided with either   1  no information about the upcoming face   2  its location   3  its expressed emotion  or  4  both its location and emotion  While cueing emotion or location resulted in faster response times than cueing unpredictive information  cueing face emotion alone resulted in faster responses than cueing face location alone  Moreover  cueing both stimulus location and emotion resulted in a superadditive reduction of response times compared with cueing location or emotion alone  suggesting that feature based attention to emotion and spatially selective attention interact to facilitate perception of face stimuli  While categorization of facial expressions was significantly affected by stable identity cues  sex and race  in the face  we found that these interactions were eliminated when uncertainty about facial expression  but not spatial uncertainty about stimulus location  was reduced by predictive cueing  This demonstrates that feature based attention to facial expression greatly attenuates the need to rely on stable identity cues to interpret facial emotion Kveraga  K   De Vito  D   Cushing  C   Im  H Y   Albohn  D N    amp  Adams  R B Jr   2019   Spatial and feature based attention to expressive faces  Experimental Brain Research  4  967 975   PDF ">
    
  </p>
</div>

<h3 id="2018">2018</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2018/10/30/publication-12.html">Sex-related differences in behavioral and amygdalar responses to compound facial threat cues</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H. Y., Adams, R. B. Jr., Cushing, C., Boshyan, J., Ward, N., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>October 30, 2018</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2018</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractDuring face perception  we integrate facial expression and eye gaze to take advantage of their shared signals  For example  fear with averted gaze provides a congruent avoidance cue  signaling both threat presence and its location  whereas fear with direct gaze sends an incongruent cue  leaving threat location ambiguous  It has been proposed that the processing of different combinations of threat cues is mediated by dual processing routes  reflexive processing via magnocellular  M  pathway and reflective processing via parvocellular  P  pathway  Because growing evidence has identified a variety of sex differences in emotional perception  here we also investigated how M and P processing of fear and eye gaze might be modulated by observer s sex  focusing on the amygdala  a structure important to threat perception and affective appraisal  We adjusted luminance and color of face stimuli to selectively engage M or P processing and asked observers to identify emotion of the face  Female observers showed more accurate behavioral responses to faces with averted gaze and greater left amygdala reactivity both to fearful and neutral faces  Conversely  males showed greater right amygdala activation only for M biased averted gaze fear faces  In addition to functional reactivity differences  females had proportionately greater bilateral amygdala volumes  which positively correlated with behavioral accuracy for M biased fear  Conversely  in males only the right amygdala volume was positively correlated with accuracy for M biased fear faces  Our findings suggest that M and P processing of facial threat cues is modulated by functional and structural differences in the amygdalae associated with observer s sex Im  H Y   Adams  R B Jr   Cushing  C   Boshyan  J   Ward  N    amp  Kveraga  K   2018   Sex related differences in behavioral and amygdalar responses to compound facial threat cues  Human Brain Mapping  39  2725 2741    PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2018/02/08/publication-11.html">Neurodynamics and connectivity during facial fear perception: The role of threat exposure and signal congruity</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Cushing, C., Im, H. Y., Adams, R. B. Jr., Ward, N., Albohn, N.D., Steiner, T.G., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>February 08, 2018</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2018</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractFearful faces convey threat cues whose meaning is contextualized by eye gaze  While averted gaze is congruent with facial fear  both signal avoidance   direct gaze  an approach signal  is incongruent with it  We have previously shown using fMRI that the amygdala is engaged more strongly by fear with averted gaze during brief exposures  However  the amygdala also responds more to fear with direct gaze during longer exposures  Here we examined previously unexplored brain oscillatory responses to characterize the neurodynamics and connectivity during brief   250ms  and longer   883ms  exposures of fearful faces with direct or averted eye gaze  We performed two experiments  one replicating the exposure time by gaze direction interaction in fMRI  N 23   and another where we confrmed greater early phase locking to averted gaze fear  congruent threat signal  with MEG  N 60  in a network of face processing regions  regardless of exposure duration  Phase locking to direct gaze fear  incongruent threat signal  then increased signifcantly for brief exposures at  350ms  and at  700ms for longer exposures  Our results characterize the stages of congruent and incongruent facial threat signal processing and show that stimulus exposure strongly afects the onset and duration of these stages Cushing  C   Im  H Y   Adams  R B Jr   Ward  N   Albohn  N D   Steiner  T G    amp  Kveraga  K   2018   Neurodynamics and connectivity during facial fear perception  The role of threat exposure and signal congruity  Scientific Reports  8  2776    PDF ">
    
  </p>
</div>

<h3 id="2017">2017</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2017/11/07/publication-9.html">Observer’s anxiety facilitates magnocellular processing of clear facial threat cues, but impairs parvocellular processing of ambiguous facial threat cues</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H. Y., Adams, R. B. Jr., Boshyan, J., Ward, N., Cushing, C., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>November 07, 2017</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2017</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractFacial expression and eye gaze provide a shared signal about threats  While a fear expression with averted gaze clearly points to the source of threat  direct gaze fear renders the source of threat ambiguous  Separable routes have been proposed to mediate these processes  with preferential attunement of the magnocellular  M  pathway to clear threat  and of the parvocellular  P  pathway to threat ambiguity  Here we investigated how observers  trait anxiety modulates M  and P pathway processing of clear and ambiguous threat cues  We scanned subjects  N  108  widely ranging in trait anxiety while they viewed fearful or neutral faces with averted or directed gaze  with the luminance and color of face stimuli calibrated to selectively engage M  or P pathways  Higher anxiety facilitated processing of clear threat projected to M pathway  but impaired perception of ambiguous threat projected to P pathway  Increased right amygdala reactivity was associated with higher anxiety for M biased averted gaze fear  while increased left amygdala reactivity was associated with higher anxiety for P biased  direct gaze fear This lateralization was more pronounced with higher anxiety Our fndings suggestthattrait anxiety diferentially afects perception of clear  averted gaze fear  and ambiguous  direct gaze fear  facial threat cues via selective engagement of M and P pathways and lateralized amygdala reactivity Im  H Y   Adams  R B Jr   Boshyan  J   Ward  N   Cushing  C    amp  Kveraga  K   2017   Observer s anxiety facilitates magnocellular processing of clear facial threat cues  but impairs parvocellular processing of ambiguous facial threat cues  Scientific Reports  7  15151   PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2017/10/30/publication-8.html">Cross-cultural and hemispheric laterality effects on the ensemble coding of emotion in facial crowds</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H. Y., Chong, S. C., Sun, J., Steiner, T. G., Albohn, D. N., Adams, R. B. Jr., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>October 30, 2017</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2017</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractIn many social situations  we make a snap judgment about crowds of people relying on their overall mood  termed  crowd emotion    Although reading crowd emotion is critical for interpersonal dynamics  the sociocultural aspects of this process have not been explored  The current study examined how culture modulates the processing of crowd emotion in Korean and American observers  Korean and American  non East Asian  participants were briefly presented with two groups of faces that were individually varying in emotional expressions and asked to choose which group between the two they would rather avoid  We found that Korean participants were more accurate than American participants overall  in line with the framework on cultural viewpoints  Holistic versus analytic processing in East Asians versus Westerners  Moreover  we found a speed advantage for otherrace crowds in both cultural groups  Finally  we found different hemispheric lateralization patterns  American participants were more accurate to perceive the facial crowd to be avoided when it was presented in the left visual field than the right visual field  indicating a right hemisphere advantage for processing crowd emotion of both European American and Korean facial crowds  However  Korean participants showed weak or nonexistent laterality effects  with a slight right hemisphere advantage for European American facial crowds and no advantage in perceiving Korean facial crowds  Instead  Korean participants showed positive emotion bias for own race faces  This work suggests that culture plays a role in modulating our crowd emotion perception of groups of faces and responses to them  Im  H Y   Chong  S C   Sun  J   Steiner  T G   Albohn  D N   Adams  R B Jr    amp  Kveraga  K   2017   Cross cultural effects on ensemble coding of emotion in facial crowds  Culture and Brain  5  125 152   PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2017/10/09/publication-10.html">Ensemble coding of crowd emotion: Differential hemispheric and visual stream contributions</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H. Y., Albohn, N.D., Steiner, T.G., Cushing, C., Adams, R.B.Jr., &amp; Kveraga, K.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>October 09, 2017</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2017</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20fMRI%22" class="tag" data-tooltip='Show items with the tag "fMRI"'>
        fMRI
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20social%20vision%22" class="tag" data-tooltip='Show items with the tag "social vision"'>
        social vision
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractIn crowds  where scrutinizing individual facial expressions is inefficient  humans can make snap judgments about the prevailing mood by reading  crowd emotion   We investigated how the brain accomplishes this feat in a set of behavioural and functional magnetic resonance imaging studies  Participants were asked to either avoid or approach one of two crowds of faces presented in the left and right visual hemifields  Perception of crowd emotion was improved when crowd stimuli contained goal congruent cues and was highly lateralized to the right hemisphere  The dorsal visual stream was preferentially activated in crowd emotion processing  with activity in the intraparietal sulcus and superior frontal gyrus predicting perceptual accuracy for crowd emotion perception  whereas activity in the fusiform cortex in the ventral stream predicted better perception of individual facial expressions  Our findings thus reveal significant behavioural differences and differential involvement of the hemispheres and the major visual streams in reading crowd versus individual face expressions Im  H Y   Albohn  N D   Steiner  T G   Cushing  C   Adams  R B Jr    amp  Kveraga  K   2017   Ensemble coding of crowd emotion  Differential hemispheric and visual stream contributions  Nature Human Behaviour  1  828 842    PDF ">
    
  </p>
</div>

<h3 id="2016">2016</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2016/06/01/publication-7.html">PsiMLE: A maximum-likelihood estimation approach to estimating psychophysical scaling and variability more reliably, efficiently, and flexibly</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Odic, D., Im, H.Y., Eisinger, R., Ly, R., &amp; Halberda, J.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>June 01, 2016</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2016</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractA simple and popular psychophysical model  usually described as overlapping Gaussian tuning curves arranged along an ordered internal scale is capable of accurately describing both human and nonhuman behavioral performance and neural coding in magnitude estimation  production  and reproduction tasks for most psychological dimensions  e g   time  space  number  or brightness   This model traditionally includes two parameters that determine how a physical stimulus is transformed into a psychological magnitude   1  an exponent that describes the compression or expansion of the physical signal into the relevant psychological scale  β   and  2  an estimate of the amount of inherent variability  often called internal noise  in the Gaussian activations along the psychological scale  σ   To date  linear slopes on log log plots have traditionally been used to estimate β  and a completely separate method of averaging coefficients of variance has been used to estimate σ  We provide a respectful  yet critical  review of these traditional methods  and offer a tutorial on a maximum likelihood estimation  MLE  and a Bayesian estimation method for estimating both β and σ  PsiMLE β σ    coupled with free software that researchers can use to implement it without a background in MLE or Bayesian statistics  R PsiMLE   We demonstrate the validity  reliability  efficiency  and flexibility of this method through a series of simulations and behavioral experiments  and find the new method to be superior to the traditional methods in all respects Odic  D   Im  H Y   Eisinger  R   Ly  R    amp  Halberda  J   2016   PsiMLE  A maximum likelihood approach to estimating psychophysical scaling and variability more reliably  efficiently  and flexibly  Behavioral Research Methods  48  445 462   PDF ">
    
  </p>
</div>

<h3 id="2015">2015</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2015/09/26/publication-7-1.html">Grouping by proximity and the visual impression of approximate number in random dot arrays</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., Zhong, S., &amp; Halberda, J.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>September 26, 2015</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2015</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20perceptual%20grouping%22" class="tag" data-tooltip='Show items with the tag "perceptual grouping"'>
        perceptual grouping
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractWe address the challenges of how to model human perceptual grouping in random dot arrays and how perceptual grouping affects human number estimation in these arrays  We introduce a modeling approach relying on a modified k means clustering algorithm to formally describe human observers  grouping behavior  We found that a default grouping window size of approximately 4  of visual angle describes human grouping judgments across a range of random dot arrays  i e   items within 4  are grouped together   This window size was highly consistent across observers and images  and was also stable across stimulus durations  suggesting that the k means model captured a robust signature of perceptual grouping  Further  the k means model outperformed other models  e g   CODE  at describing human grouping behavior  Next  we found that the more the dots in a display are clustered together  the more human observers tend to underestimate the numerosity of the dots  We demonstrate that this effect is independent of density  and the modified k means model can predict human observers  numerosity judgments and underestimation  Finally  we explored the robustness of the relationship between clustering and dot number underestimation and found that the effects of clustering remain  but are greatly reduced  when participants receive feedback on every trial  Together  this work suggests some promising avenues for formal models of human grouping behavior  and it highlights the importance of a 4  window of perceptual grouping  Lastly  it reveals a robust  somewhat plastic  relationship between perceptual grouping and number estimation Im  H Y   Zhong  S    amp  Halberda  J   2016   Perceptual groups as a unit for rapid extraction of approximate number of elements in random dot arrays  Vision Research  126  291 307    PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2015/09/26/publication-7-2.html">Long lasting attentional-context dependent visuomotor memory.</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., Bédard, P., &amp; Song, J-H.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>September 26, 2015</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2015</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20visuomotor%20learning%22" class="tag" data-tooltip='Show items with the tag "visuomotor learning"'>
        visuomotor learning
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20hand%20movement%20tracking%22" class="tag" data-tooltip='Show items with the tag "hand movement tracking"'>
        hand movement tracking
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractUsing a dual task paradigm  we recently reported that visuomotor adaptation acquired under distraction of a secondary attention demanding discrimination task could be remembered only when a similar distraction was present  In contrast  when tested without the distracting task  performance reverted to untrained levels  Song  amp  Bédard  2015   Here  we demonstrated that this newfound paradoxical benefit of consistent dual task context lasts over 1 day  such that visuomotor memory retrieval is enhanced under conditions where it is more difficult to engage in attentional selection of the motor task  Furthermore  this long term effect was evident even when the task type or sensory modality of the secondary task differed between initial adaptation and the delayed recall on the next day  We conclude that attentional diversion by performing a dual task forms a long term vital context for visuomotor memory independent of external contexts without taxing capacity limited attention Im  H Y   Bédard  P    amp  Song  J H   2016   Long lasting attentional context dependent visuomotor memory  Journal of Experimental Psychology  Human Perception  amp  Performance  42  1269 1274   PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2015/06/01/publication-6.html">Encoding attentional states during visuomotor adaptation</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., Bédard, P., &amp; Song, J-H.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>June 01, 2015</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2015</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20visuomotor%20learning%22" class="tag" data-tooltip='Show items with the tag "visuomotor learning"'>
        visuomotor learning
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20hand%20movement%20tracking%22" class="tag" data-tooltip='Show items with the tag "hand movement tracking"'>
        hand movement tracking
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractWe recently showed that visuomotor adaptation acquired under attentional distraction is better recalled under a similar level of distraction compared to no distraction  This paradoxical effect suggests that attentional state  e g   divided or undivided  is encoded as an internal context during visuomotor learning and should be reinstated for successful recall  Song  amp  Bedard    2015   To investigate if there is a critical temporal window for encoding attentional state in visuomotor memory  we manipulated whether participants performed the secondary attentiondemanding task concurrently in the early or late phase of visuomotor learning  Recall performance was enhanced when the attentional states between recall and the early phase of visuomotor learning were consistent  However  it reverted to untrained levels when tested under the attentional state of the late phase learning  This suggests that attentional state is primarily encoded during the early phase of learning before motor errors decrease and reach an asymptote  Furthermore  we demonstrate that when divided and undivided attentional states were mixed during visuomotor adaptation  only divided attention was encoded as an internal cue for memory retrieval  Therefore  a single attentional state appears to be primarily integrated with visuomotor memory while motor error reduction is in progress during learning Im  H Y   Bédard  P    amp  Song  J H   2015   Encoding attentional states during visuomotor learning  Journal of Vision  15  1 16   PDF ">
    
  </p>
</div>

<h3 id="2014">2014</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2014/12/08/publication-5.html">Ensemble statistics as units of selection</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., Park, W., &amp; Chong, S.C.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>December 08, 2014</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2014</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20perceptual%20grouping%22" class="tag" data-tooltip='Show items with the tag "perceptual grouping"'>
        perceptual grouping
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractThe current study investigated whether attentional mechanisms operate on ensembles as higher order units for selection  In Experiment 1  we presented sets of circles and asked participants to compare the mean sizes of the sets while concurrently detecting a small probe appearing at a centroid of one of the sets  We found that  both with and even without the task instruction to favour larger mean sizes  people s mean size judgement was more accurate for the sets with larger mean sizes  In addition  detection of the probe appearing in the set with the largest mean size was facilitated by a matching task instruction  However  when the task instruction favoured smaller mean sizes  mean size judgement became more accurate for the sets with smaller mean sizes  These results suggest that attentional selection can be based on ensembles  In Experiment 2  we found further evidence that attention was directed towards the centroid of an ensemble  rather than towards an individual member of the ensemble  Together  these results suggest that attentional modulation can operate at the level of ensembles instead of selecting individuals separately and that the centroid of an ensemble can be the locus of selection based on an ensemble Im  H Y   Park  W    amp  Chong  S C   2015   Ensemble statistics as a unit of selection  Journal of Cognitive Psychology  27  114 127   PDF ">
    
  </p>
</div>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2014/01/01/publication-4.html">Mean size as a unit of visual working memory</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., &amp;  Chong, S.C.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>January 01, 2014</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2014</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20perceptual%20grouping%22" class="tag" data-tooltip='Show items with the tag "perceptual grouping"'>
        perceptual grouping
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractVisual environments often contain multiple elements  some of which are similar to one another or spatially grouped together  In the current study we investigated how one can use perceptual groups in representing ensemble features of the groups  In experiment I we found that participants  performance improved when items were easily segmented by a grouping cue based on proximity  suggesting that spatial grouping facilitates extracting and remembering ensemble representations from visual arrays consisting of multiple elements  In experiment 2 we found that spatial grouping improved performance only when the grouped subsets were tested for the memory task  whereas it impaired performance when other subsets that were not grouped were tested  suggesting that the benefit from grouping may come from better extraction for storage  rather than later decision processes such as accessibility  Taken together  our results suggest that perceptual grouping of multiple items by proximity facilitates extraction of ensemble statistics from groups of items  enhancing visual memory of the ensembles in a visual array Im  H Y    amp  Chong  S C  2014   Mean size as a unit of visual working memory  Perception  43  663 676   PDF ">
    
  </p>
</div>

<h3 id="2012">2012</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2012/11/01/publication-3.html">The effects of sampling and internal noise on the representation of ensemble average size</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., &amp;  Halberda, J.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>November 01, 2012</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2012</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractIncreasing numbers of studies have explored human observers  ability to rapidly extract statistical descriptions from collections of similar items  e g   the average size and orientation of a group of tilted Gabor patches   Determining whether these descriptions are generated by mechanisms that are independent from object based sampling procedures requires that we investigate how internal noise  external noise  and sampling affect subjects  performance  Here we systematically manipulated the external variability of ensembles and used variance summation modeling to estimate both the internal noise and the number of samples that affected the representation of ensemble average size  The results suggest that humans sample many more than one or two items from an array when forming an estimate of the average size  and that the internal noise that affects ensemble processing is lower than the noise that affects the processing of single objects  These results are discussed in light of other recent modeling efforts and suggest that ensemble processing of average size relies on a mechanism that is distinct from segmenting individual items  This ensemble process may be more similar to texture processing Im  H Y    amp  Halberda  J   2013   The effects of sampling and internal noise on the representation of ensemble average size  Attention  Perception   amp  Psychophysics  75  278 286   PDF ">
    
  </p>
</div>

<h3 id="2009">2009</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2009/02/01/publication-2.html">Computation of mean size is based on perceived size</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Im, H.Y., &amp;  Chong, S.C.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>February 01, 2009</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2009</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20ensemble%20perception%22" class="tag" data-tooltip='Show items with the tag "ensemble perception"'>
        ensemble perception
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="AbstractThe present study investigated whether computation of mean object size was based on perceived or physicalsize  The Ebbinghaus illusion was used to make the perceived size of a circle different from its physical size Four Ebbinghaus configurations were presented either simultaneously  Experiment 1  or sequentially  Experiment 2  to each visual field  and participants were instructed to attend only to the central circles of each configuration  Participants  judgments of mean central circle size were influenced by the Ebbinghaus illusion  Inaddition  the Ebbinghaus illusion influenced the coding of individual size rather than the averaging  These resultssuggest that perceived rather than physical size was used in computing the mean size Im  H Y    amp  Chong  S C  2009   Computation of mean size is based on perceived size  Attention  Perception   amp  Psychophysics 71  375 384  PDF ">
    
  </p>
</div>

<h3 id="2007">2007</h3>

<div class="post-excerpt">
  
  
  <a href="/preview/pr-2/2007/12/30/publication-1.html">The Influence of Depth Context on Blind Spot Filling-in</a>

  <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Park, K.M.,  Cha, O.,  Kim, S.,  Im, H.Y.,  Chong, S.C.</span>
      </span>
    
  

  
  

  <!-- Remove the date last updated parameter
  
  -->

  <!-- Old code for publishing website with blog format (monday day, year)
  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>December 30, 2007</span>
    </span>
  
  -->

<!-- New code for just year format-->

  <span data-tooltip="Originally published on">
    <i class="icon fa-regular fa-calendar"></i>
    <span>2007</span>
  </span>

  

  
</div>

<!-- 
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span></span>
    </span>
-->


  


  <div class="tags" data-link="/preview/pr-2/publication">
    
      <a href="/preview/pr-2/publication?search=%22tag:%20psychophysics%22" class="tag" data-tooltip='Show items with the tag "psychophysics"'>
        psychophysics
      </a>
    
      <a href="/preview/pr-2/publication?search=%22tag:%20perceptual%20grouping%22" class="tag" data-tooltip='Show items with the tag "perceptual grouping"'>
        perceptual grouping
      </a>
    
  </div>






  <!-- Removed post info preview, see original template for code (if need to restore)-->
  
  <p data-search="This study was published in Korean  Hangul  under the Korean Journal of Cognitive Science  Volume 18  The abstract has been translated here for convenience  AbstractThis study verified the effects of bottom up and top down information on blind spot filling through two psychophysical experiments  In both experiments  two stimuli with the same probability of filling the blind spot were presented to the blind spot  When these two stimuli compete to fill the blind spot  the depth context of one of the stimuli was manipulated to examine the interaction between upward and downward factors  Depth context was defined as the relative depth between the stimuli filled in the blind spot and the stimuli around the blind spot  and participants reported the perceptual depth of the target filled in the blind spot  As a result of the experiment  it was found that the blind spots were mostly filled with stimuli with added depth context  which is top down information  rather than stimuli with only bottom up information  Through controlled experiments  it was found that this depth perception was not due to the resolution of peripheral vision where the blind spot was located  The above experimental results suggest that the top down processing process  such as depth context information  also affects the blind spot filling process Park  K M   Cha  O   Kim  S   Im  H Y   Chong  S C    2007   The Influence of Depth Context on Blind Spot Filling in  Korean Journal of Cognitive Science  18  351 370 ">
    
  </p>
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/preview/pr-2/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0001-9379-1347" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=Zq3Z-ioAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://www.bcchr.ca/research" data-tooltip="BC Children's Hospital webpage" data-style="bare" aria-label="BC Children's Hospital webpage">
      <i class="icon fa-solid fa-hospital"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://psych.ubc.ca/" data-tooltip="University of British Columbia Psychology" data-style="bare" aria-label="University of British Columbia Psychology">
      <i class="icon fa-solid fa-database"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://vision.ubc.ca/" data-tooltip="UBC Vision" data-style="bare" aria-label="UBC Vision">
      <i class="icon fa-regular fa-eye"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://languagesciences.ubc.ca/" data-tooltip="UBC Language Sciences Institute" data-style="bare" aria-label="UBC Language Sciences Institute">
      <i class="icon fa-solid fa-globe"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://www.centreforbrainhealth.ca/" data-tooltip="Centre for Brain Health" data-style="bare" aria-label="Centre for Brain Health">
      <i class="icon fa-solid fa-microscope"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2023
    IMM Laboratory
      |   Template:
    <a href="https://github.com/greenelab/lab-website-template">
      Greene Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
